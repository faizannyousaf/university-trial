{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "140ff12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a12ffcf",
   "metadata": {},
   "source": [
    "## Defining paths and files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "90b9585b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './Stress-Predict-Dataset-main/Stress-Predict-Dataset-main/Raw_data/'\n",
    "folders = os.listdir(data_dir)[1:]\n",
    "temp_file = '/TEMP.csv'\n",
    "bvp_file = '/BVP.csv'\n",
    "acc_file = '/ACC.csv'\n",
    "eda_file = '/EDA.csv'\n",
    "info_file = '/info.txt'\n",
    "ibi_file = '/IBI.csv'\n",
    "hr_file = '/HR.csv'\n",
    "\n",
    "# chosen_signal_files = [temp_file, bvp_file, eda_file, hr_file, ibi_file]\n",
    "chosen_signal_files = [temp_file, bvp_file, eda_file, hr_file]\n",
    "# The accelerometer data is skipped since the hand moves while performing activities and is not a reliable signal\n",
    "# for stress detection. All other signals are explored\n",
    "\n",
    "# Exploration of the frequency of ibi signals reveals that the signals are not reliably consistent. Within 5 second\n",
    "# windows, an average of 6 values is found however it is often 5 or 7 values. While this is alright and can be \n",
    "# dealt with, the value often falls to 0 which means no ibi values are recoreded in intervals longer than 5 seconds\n",
    "# This motivated the decision to drop ibi as a feature as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd097ac0",
   "metadata": {},
   "source": [
    "## Initial exploration to fix timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "0faa7514",
   "metadata": {},
   "outputs": [],
   "source": [
    "markers = extract_starts_ends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "2099c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject:  1\n",
      "2023-04-19 09:22:00\n",
      "2023-04-19 09:27:41\n",
      "2023-04-19 09:27:41\n",
      "2023-04-19 09:27:41\n",
      "2023-04-19 09:27:51\n",
      "2023-04-19 09:27:41\n",
      "----------------------------------------\n",
      "Subject:  2\n",
      "2023-04-19 09:48:00\n",
      "2023-04-19 09:52:54\n",
      "2023-04-19 09:52:54\n",
      "2023-04-19 09:52:54\n",
      "2023-04-19 09:53:04\n",
      "2023-04-19 09:52:54\n",
      "----------------------------------------\n",
      "Subject:  3\n",
      "2023-04-19 10:50:00\n",
      "2023-04-19 10:56:12\n",
      "2023-04-19 10:56:12\n",
      "2023-04-19 10:56:12\n",
      "2023-04-19 10:56:22\n",
      "2023-04-19 10:56:12\n",
      "----------------------------------------\n",
      "Subject:  4\n",
      "2023-04-19 11:25:00\n",
      "2023-04-19 11:29:49\n",
      "2023-04-19 11:29:49\n",
      "2023-04-19 11:29:49\n",
      "2023-04-19 11:29:59\n",
      "2023-04-19 11:29:49\n",
      "----------------------------------------\n",
      "Subject:  5\n",
      "2023-04-19 09:05:00\n",
      "2023-04-19 09:12:05\n",
      "2023-04-19 09:12:05\n",
      "2023-04-19 09:12:05\n",
      "2023-04-19 09:12:15\n",
      "2023-04-19 09:12:05\n",
      "----------------------------------------\n",
      "Subject:  6\n",
      "2023-04-19 09:41:00\n",
      "2023-04-19 09:45:00\n",
      "2023-04-19 09:45:00\n",
      "2023-04-19 09:45:00\n",
      "2023-04-19 09:45:10\n",
      "2023-04-19 09:45:00\n",
      "----------------------------------------\n",
      "Subject:  7\n",
      "2023-04-19 10:25:00\n",
      "2023-04-19 10:30:34\n",
      "2023-04-19 10:30:34\n",
      "2023-04-19 10:30:34\n",
      "2023-04-19 10:30:44\n",
      "2023-04-19 10:30:34\n",
      "----------------------------------------\n",
      "Subject:  8\n",
      "2023-04-19 12:08:00\n",
      "2023-04-19 12:12:24\n",
      "2023-04-19 12:12:24\n",
      "2023-04-19 12:12:24\n",
      "2023-04-19 12:12:34\n",
      "2023-04-19 12:12:24\n",
      "----------------------------------------\n",
      "Subject:  9\n",
      "2023-04-19 12:34:00\n",
      "2023-04-19 12:38:09\n",
      "2023-04-19 12:38:09\n",
      "2023-04-19 12:38:09\n",
      "2023-04-19 12:38:19\n",
      "2023-04-19 12:38:09\n",
      "----------------------------------------\n",
      "Subject:  10\n",
      "2023-04-19 13:11:00\n",
      "2023-04-19 13:14:19\n",
      "2023-04-19 13:14:19\n",
      "2023-04-19 13:14:19\n",
      "2023-04-19 13:14:29\n",
      "2023-04-19 13:14:19\n",
      "----------------------------------------\n",
      "Subject:  11\n",
      "2023-04-19 15:15:00\n",
      "2023-04-19 15:20:28\n",
      "2023-04-19 15:20:28\n",
      "2023-04-19 15:20:28\n",
      "2023-04-19 15:20:38\n",
      "2023-04-19 15:20:28\n",
      "----------------------------------------\n",
      "Subject:  12\n",
      "2023-04-19 09:09:00\n",
      "2023-04-19 09:15:08\n",
      "2023-04-19 09:15:08\n",
      "2023-04-19 09:15:08\n",
      "2023-04-19 09:15:18\n",
      "2023-04-19 09:15:08\n",
      "----------------------------------------\n",
      "Subject:  13\n",
      "2023-04-19 11:20:00\n",
      "2023-04-19 11:24:07\n",
      "2023-04-19 11:24:07\n",
      "2023-04-19 11:24:07\n",
      "2023-04-19 11:24:17\n",
      "2023-04-19 11:24:07\n",
      "----------------------------------------\n",
      "Subject:  14\n",
      "2023-04-19 11:41:00\n",
      "2023-04-19 11:47:43\n",
      "2023-04-19 11:47:43\n",
      "2023-04-19 11:47:43\n",
      "2023-04-19 11:47:53\n",
      "2023-04-19 11:47:43\n",
      "----------------------------------------\n",
      "Subject:  15\n",
      "2023-04-19 12:31:00\n",
      "2023-04-19 12:36:29\n",
      "2023-04-19 12:36:29\n",
      "2023-04-19 12:36:29\n",
      "2023-04-19 12:36:39\n",
      "2023-04-19 12:36:29\n",
      "----------------------------------------\n",
      "Subject:  16\n",
      "2023-04-19 13:05:00\n",
      "2023-04-19 13:12:26\n",
      "2023-04-19 13:12:26\n",
      "2023-04-19 13:12:26\n",
      "2023-04-19 13:12:36\n",
      "2023-04-19 13:12:26\n",
      "----------------------------------------\n",
      "Subject:  17\n",
      "2023-04-19 15:09:00\n",
      "2023-04-19 15:12:49\n",
      "2023-04-19 15:12:49\n",
      "2023-04-19 15:12:49\n",
      "2023-04-19 15:12:59\n",
      "2023-04-19 15:12:49\n",
      "----------------------------------------\n",
      "Subject:  18\n",
      "2023-04-19 15:30:00\n",
      "2023-04-19 15:34:45\n",
      "2023-04-19 15:34:45\n",
      "2023-04-19 15:34:45\n",
      "2023-04-19 15:34:55\n",
      "2023-04-19 15:34:45\n",
      "----------------------------------------\n",
      "Subject:  19\n",
      "2023-04-19 09:15:00\n",
      "2023-04-19 09:18:50\n",
      "2023-04-19 09:18:50\n",
      "2023-04-19 09:18:50\n",
      "2023-04-19 09:19:00\n",
      "2023-04-19 09:18:50\n",
      "----------------------------------------\n",
      "Subject:  20\n",
      "2023-04-19 09:46:00\n",
      "2023-04-19 09:49:49\n",
      "2023-04-19 09:49:49\n",
      "2023-04-19 09:49:49\n",
      "2023-04-19 09:49:59\n",
      "2023-04-19 09:49:49\n",
      "----------------------------------------\n",
      "Subject:  21\n",
      "2023-04-19 11:40:00\n",
      "2023-04-19 11:43:57\n",
      "2023-04-19 11:43:57\n",
      "2023-04-19 11:43:57\n",
      "2023-04-19 11:44:07\n",
      "2023-04-19 11:43:57\n",
      "----------------------------------------\n",
      "Subject:  22\n",
      "2023-04-19 12:16:00\n",
      "2023-04-19 12:18:43\n",
      "2023-04-19 12:18:43\n",
      "2023-04-19 12:18:43\n",
      "2023-04-19 12:18:53\n",
      "2023-04-19 12:18:43\n",
      "----------------------------------------\n",
      "Subject:  23\n",
      "2023-04-19 12:41:00\n",
      "2023-04-19 12:44:29\n",
      "2023-04-19 12:44:29\n",
      "2023-04-19 12:44:29\n",
      "2023-04-19 12:44:39\n",
      "2023-04-19 12:44:29\n",
      "----------------------------------------\n",
      "Subject:  24\n",
      "2023-04-19 14:36:00\n",
      "2023-04-19 14:39:41\n",
      "2023-04-19 14:39:41\n",
      "2023-04-19 14:39:41\n",
      "2023-04-19 14:39:51\n",
      "2023-04-19 14:39:41\n",
      "----------------------------------------\n",
      "Subject:  25\n",
      "2023-04-19 15:09:00\n",
      "2023-04-19 15:12:29\n",
      "2023-04-19 15:12:29\n",
      "2023-04-19 15:12:29\n",
      "2023-04-19 15:12:39\n",
      "2023-04-19 15:12:29\n",
      "----------------------------------------\n",
      "Subject:  26\n",
      "2023-04-19 15:37:00\n",
      "2023-04-19 15:41:44\n",
      "2023-04-19 15:41:44\n",
      "2023-04-19 15:41:44\n",
      "2023-04-19 15:41:54\n",
      "2023-04-19 15:41:44\n",
      "----------------------------------------\n",
      "Subject:  27\n",
      "2023-04-19 09:40:00\n",
      "2023-04-19 09:45:45\n",
      "2023-04-19 09:45:45\n",
      "2023-04-19 09:45:45\n",
      "2023-04-19 09:45:55\n",
      "2023-04-19 09:45:45\n",
      "----------------------------------------\n",
      "Subject:  28\n",
      "2023-04-19 12:13:00\n",
      "2023-04-19 12:16:26\n",
      "2023-04-19 12:16:26\n",
      "2023-04-19 12:16:26\n",
      "2023-04-19 12:16:36\n",
      "2023-04-19 12:16:26\n",
      "----------------------------------------\n",
      "Subject:  29\n",
      "2023-04-19 12:34:00\n",
      "2023-04-19 12:39:29\n",
      "2023-04-19 12:39:29\n",
      "2023-04-19 12:39:29\n",
      "2023-04-19 12:39:39\n",
      "2023-04-19 12:39:29\n",
      "----------------------------------------\n",
      "Subject:  30\n",
      "2023-04-19 12:53:00\n",
      "2023-04-19 12:59:33\n",
      "2023-04-19 12:59:33\n",
      "2023-04-19 12:59:33\n",
      "2023-04-19 12:59:43\n",
      "2023-04-19 12:59:33\n",
      "----------------------------------------\n",
      "Subject:  31\n",
      "2023-04-19 15:05:00\n",
      "2023-04-19 15:08:48\n",
      "2023-04-19 15:08:48\n",
      "2023-04-19 15:08:48\n",
      "2023-04-19 15:08:58\n",
      "2023-04-19 15:08:48\n",
      "----------------------------------------\n",
      "Subject:  32\n",
      "2023-04-19 15:34:00\n",
      "2023-04-19 15:36:19\n",
      "2023-04-19 15:36:19\n",
      "2023-04-19 15:36:19\n",
      "2023-04-19 15:36:29\n",
      "2023-04-19 15:36:19\n",
      "----------------------------------------\n",
      "Subject:  33\n",
      "2023-04-19 12:11:00\n",
      "2023-04-19 12:14:20\n",
      "2023-04-19 12:14:20\n",
      "2023-04-19 12:14:20\n",
      "2023-04-19 12:14:30\n",
      "2023-04-19 12:14:20\n",
      "----------------------------------------\n",
      "Subject:  34\n",
      "2023-04-19 14:30:00\n",
      "2023-04-19 14:36:34\n",
      "2023-04-19 14:36:34\n",
      "2023-04-19 14:36:34\n",
      "2023-04-19 14:36:44\n",
      "2023-04-19 14:36:34\n",
      "----------------------------------------\n",
      "Subject:  35\n",
      "2023-04-19 15:07:00\n",
      "2023-04-19 15:12:04\n",
      "2023-04-19 15:12:04\n",
      "2023-04-19 15:12:04\n",
      "2023-04-19 15:12:14\n",
      "2023-04-19 15:12:04\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# This code block was used to help correcting the incorrect timestamps in the excel file\n",
    "for subject_idx in range(1,36):\n",
    "    print(\"Subject: \", subject_idx)\n",
    "    print(markers[0][subject_idx])\n",
    "    for file in chosen_signal_files:\n",
    "        print(get_signal_start(read_data(subject_idx, file)[0][0]))\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1dea8",
   "metadata": {},
   "source": [
    "## Helper functions for data loading and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "bb7cd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tags(subject_idx):\n",
    "    '''\n",
    "    This function reads the tags for a given subject and returns the UNIX timestamp of the start of the session \n",
    "    followed by the start and end timestamps of each of the three stress inducing sesssions the participant goes\n",
    "    through. \n",
    "    THIS FUNCTION HAS NOT BEEN USED DUE TO INCONSISTENCIES IN TAGS IN RAW DATA!\n",
    "    '''\n",
    "    tag_data = pd.read_csv(data_dir + 'S' + (str(subject_idx) if subject_idx > 9 else '0' + str(subject_idx)) + '/tags_' + 'S' + (str(subject_idx) if subject_idx > 9 else '0' + str(subject_idx)) + '.csv', header=None)\n",
    "    tag_data = tag_data.to_numpy()\n",
    "    start = tag_data[0]\n",
    "    stroop_start, stroop_end = tag_data[2], tag_data[3]\n",
    "    trier_start, trier_end = tag_data[4], tag_data[5]\n",
    "    hyperv_start, hyperv_end = tag_data[6], tag_data[7]\n",
    "    return [start, (stroop_start, stroop_end), (trier_start, trier_end), (hyperv_start, hyperv_end)]\n",
    "\n",
    "def read_data(subject_idx, file):\n",
    "    '''\n",
    "    This function reads all signal files.\n",
    "    Returns (UNIX timestamp for start of signal acquisition, sampling frequency of sensor, acquired data)\n",
    "    In case of reading Inter-beat interval data, returns (UNIX timestamp for start, arr([seconds since start, duration]))\n",
    "    '''    \n",
    "    data = pd.read_csv(data_dir + 'S' + (str(subject_idx) if subject_idx > 9 else '0' + str(subject_idx)) + file, header=None).to_numpy()\n",
    "    \n",
    "    if file == '/IBI.csv':\n",
    "        ibi_start = data[0, 0]\n",
    "        ibi_data = data[1:]\n",
    "        return ([ibi_start], 0, ibi_data)\n",
    "    \n",
    "    data_start, data_sfreq = data[0], data[1]\n",
    "    return (data_start, data_sfreq, data[2:])\n",
    "\n",
    "def convert_to_timestamp(time_str):\n",
    "    '''This function converts timestamps from excel file into Pandas Timestamp objects.'''\n",
    "    hours = time_str[:2]\n",
    "    minutes = time_str[3:5]\n",
    "    return pd.Timestamp(hours + ':' + minutes)    \n",
    "\n",
    "def extract_starts_ends(return_from_beg = False):\n",
    "    '''This function extracts start and end times for stress inducing events from excel file.'''\n",
    "    timestamps = pd.read_excel(data_dir + 'Time_logs.xlsx')\n",
    "    \n",
    "    file_starts, file_ends = timestamps['Start Time'].astype(str)[1:], timestamps['End Time'].astype(str)[1:]\n",
    "    stroop_starts, stroop_ends = timestamps['Stroop Test'].astype(str)[1:], timestamps['Unnamed: 9'].astype(str)[1:]\n",
    "    int_starts, int_ends = timestamps['Interview'].astype(str)[1:], timestamps['Unnamed: 13'].astype(str)[1:]\n",
    "    hypv_starts, hypv_ends = timestamps['Hyperventilation'].astype(str)[1:], timestamps['Unnamed: 17'].astype(str)[1:]\n",
    "    \n",
    "    file_starts, file_ends = file_starts.map(convert_to_timestamp), file_ends.map(convert_to_timestamp)\n",
    "    stroop_starts, stroop_ends = stroop_starts.map(convert_to_timestamp), stroop_ends.map(convert_to_timestamp)\n",
    "    int_starts, int_ends = int_starts.map(convert_to_timestamp), int_ends.map(convert_to_timestamp)\n",
    "    hypv_starts, hypv_ends = hypv_starts.map(convert_to_timestamp), hypv_ends.map(convert_to_timestamp)\n",
    "    \n",
    "    stress_event_markers = [file_starts, file_ends, stroop_starts, stroop_ends, int_starts, int_ends, hypv_starts, hypv_ends]\n",
    "    \n",
    "    if return_from_beg:\n",
    "        stress_event_markers_from_beg = [file_starts, file_ends]\n",
    "        for markers in stress_event_markers[1:]:\n",
    "            stress_event_markers_from_beg.append(markers-file_starts)\n",
    "\n",
    "        return stress_event_markers_from_beg\n",
    "    \n",
    "    else:\n",
    "        return stress_event_markers\n",
    "\n",
    "def get_signal_start(unix_start):\n",
    "    '''This function converts a UNIX timestamp into Pandas Timestamp only preserving the time information.'''\n",
    "    time = datetime.datetime.fromtimestamp(unix_start)\n",
    "    return pd.Timestamp(str(time.hour) + ':' + (str(time.minute) if time.minute > 9 else '0' + str(time.minute)) + ':' + str(time.second))\n",
    "\n",
    "def generate_signal_df(subject_idx, signal_file):\n",
    "    '''This function generates a Pandas Series for each signal.'''\n",
    "    data = read_data(subject_idx, signal_file)\n",
    "    start_time = get_signal_start(data[0][0])\n",
    "    periods = len(data[2])\n",
    "    \n",
    "    if signal_file == temp_file or signal_file == eda_file:\n",
    "        freq = '250ms'\n",
    "    elif signal_file == bvp_file:\n",
    "        freq = '15625us'\n",
    "    elif signal_file == hr_file:\n",
    "        freq = '1S'\n",
    "    else:\n",
    "        time_idx = []\n",
    "        for i in range(len(data[2])):\n",
    "            time_idx.append(start_time + pd.Timedelta(seconds = (data[2][i][0])))\n",
    "        ibi_df = pd.DataFrame(data[2][:,1], index = time_idx)\n",
    "        return ibi_df\n",
    "        \n",
    "    time_index = pd.date_range(start = start_time, periods=periods, freq=freq)\n",
    "    df = pd.DataFrame(data[2])\n",
    "    df.index = time_index\n",
    "    return df\n",
    "\n",
    "def gen_label_df(subject_idx, event_markers, epoch_length=5):\n",
    "    '''This function generates labels for each epoch. Epoch_length is provided in seconds.'''\n",
    "    event_marks = []\n",
    "    for i in range(len(event_markers)):\n",
    "        event_marks.append(event_markers[i][subject_idx])\n",
    "    freq = str(epoch_length) + 'S'\n",
    "    time_index = pd.date_range(start = event_marks[0], end = event_marks[1], freq=freq)\n",
    "    label_df = pd.DataFrame(np.zeros(len(time_index)), index=time_index, columns=['Label'])\n",
    "    for i in range(1,4):\n",
    "        idx = label_df.between_time(event_marks[2*i].time(), event_marks[2*i+1].time()).index\n",
    "        label_df.loc[idx, 'Label'] = 1\n",
    "    return label_df\n",
    "    \n",
    "def gen_epoched_data(subject_idx, markers=extract_starts_ends(),chosen_signal_files=chosen_signal_files, use_scaling=False):\n",
    "    '''This function uses all methods defined above to generate epoched data with signals for a given subject.'''\n",
    "    signal_dfs = []\n",
    "    for file in chosen_signal_files:\n",
    "        signal_df = generate_signal_df(subject_idx, file)\n",
    "        if use_scaling:\n",
    "            scaler = StandardScaler()\n",
    "            idx = signal_df.index\n",
    "            signal_df = pd.DataFrame(scaler.fit_transform(signal_df), index=idx)\n",
    "        signal_dfs.append(signal_df)\n",
    "    label_df = gen_label_df(subject_idx, markers)\n",
    "    epoch_timestamps = label_df.index\n",
    "    X, y = [], []\n",
    "    for i in range(len(epoch_timestamps) - 1):\n",
    "        signals = []\n",
    "        for j in range(len(chosen_signal_files)):\n",
    "            signals = np.concatenate((signals,\n",
    "                                  (signal_dfs[j].between_time(epoch_timestamps[i].time(), \n",
    "                                                            epoch_timestamps[i+1].time()).to_numpy().reshape(-1))),\n",
    "                                   axis=0)\n",
    "        if len(signals) == 369:\n",
    "            X.append(signals)\n",
    "            y.append(label_df.loc[epoch_timestamps[i], 'Label'])\n",
    "    if len(X) == 0:\n",
    "        print(\"No epochs returned. Please check to see if the length of signals should still be 369. If not, modify this function. Subject#: \", subject_idx)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b527aac",
   "metadata": {},
   "source": [
    "## Loading all data without scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "id": "6205100f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.empty((0, 369)), np.empty((0))\n",
    "for i in range(1, 36):\n",
    "    if i != 17:\n",
    "        _x, _y = gen_epoched_data(i)\n",
    "        x = np.concatenate((x, np.array(_x)), axis=0)\n",
    "        y = np.concatenate((y, np.array(_y)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "id": "98a54de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.array(x), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 699,
   "id": "536c8216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22207, 369) (22207,)\n"
     ]
    }
   ],
   "source": [
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6dd286",
   "metadata": {},
   "source": [
    "## Training a Decision Tree Classifier with 0.25 test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 700,
   "id": "9b9fb7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 700,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y)\n",
    "tree.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 701,
   "id": "0ff30190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.86      0.86      3697\n",
      "         1.0       0.72      0.69      0.70      1855\n",
      "\n",
      "    accuracy                           0.81      5552\n",
      "   macro avg       0.78      0.78      0.78      5552\n",
      "weighted avg       0.80      0.81      0.81      5552\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = tree.predict(x_test)\n",
    "print(classification_report(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 702,
   "id": "7b23ac37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3190  507]\n",
      " [ 570 1285]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(preds, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "id": "9daaf649",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8060158501440923"
      ]
     },
     "execution_count": 713,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e9efb",
   "metadata": {},
   "source": [
    "## Cross-Validation on Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 765,
   "id": "1279d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(tree, x, y, cv=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 766,
   "id": "950c8143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.63004323 0.63814841 0.64355187 0.63502072]\n",
      "Average score with cross-validation:  0.6366910582095185\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(\"Average score with cross-validation: \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5b7293",
   "metadata": {},
   "source": [
    "## Loading data with scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "id": "d8f8b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = np.empty((0, 369)), np.empty((0))\n",
    "for i in range(1, 36):\n",
    "    if i != 17:\n",
    "        _x, _y = gen_epoched_data(i, use_scaling=True)\n",
    "        x = np.concatenate((x, np.array(_x)), axis=0)\n",
    "        y = np.concatenate((y, np.array(_y)), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a5581",
   "metadata": {},
   "source": [
    "## Defining MLP and training with 0.25 test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 767,
   "id": "c5470bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', metrics='acc', loss=BinaryCrossentropy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 768,
   "id": "42823d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "db34928d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f73e44cfeb0>"
      ]
     },
     "execution_count": 769,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=300, batch_size=4096, validation_split=0.15, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 770,
   "id": "ac955aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174/174 [==============================] - 0s 728us/step - loss: 2.4937 - acc: 0.6922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.4936625957489014, 0.6921830177307129]"
      ]
     },
     "execution_count": 770,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b905e1",
   "metadata": {},
   "source": [
    "## Cross-validation with MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 763,
   "id": "2e9ab26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139/139 [==============================] - 0s 692us/step - loss: 2.9436 - acc: 0.7861\n",
      "139/139 [==============================] - 0s 708us/step - loss: 2.8932 - acc: 0.7393\n",
      "139/139 [==============================] - 0s 718us/step - loss: 1.0946 - acc: 0.8782\n",
      "139/139 [==============================] - 0s 753us/step - loss: 0.4361 - acc: 0.9286\n",
      "139/139 [==============================] - 0s 703us/step - loss: 0.6983 - acc: 0.9124\n",
      "[0.7861323952674866, 0.739306628704071, 0.8781805634498596, 0.9286196827888489, 0.9124071002006531]\n",
      "Average score after cross validation:  0.8489292740821839\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "n_split=5\n",
    "scores = []\n",
    "\n",
    "for train_index,test_index in KFold(n_split).split(x):\n",
    "    x_train, x_test = x[train_index],x[test_index]\n",
    "    y_train, y_test = y[train_index],y[test_index]\n",
    "\n",
    "    model.fit(x_train, y_train,epochs=300, batch_size=4096, verbose=False)\n",
    "\n",
    "    scores.append(model.evaluate(x_test,y_test)[1])\n",
    "print(scores)\n",
    "print(\"Average score after cross validation: \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d906dba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
